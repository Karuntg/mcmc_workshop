{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules and make matplotlib plot inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import emcee\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Read in our sample dataset and visualize it with matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y,y_err = np.loadtxt('sample_data.csv',delimiter=',',unpack=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that we read in reasonable values\n",
    "print(x)\n",
    "print(y)\n",
    "print(y_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize it\n",
    "plt.figure()\n",
    "plt.errorbar(x,y,yerr=y_err,fmt='bo');\n",
    "plt.xlabel('x');\n",
    "plt.ylabel('y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Creating a model\n",
    "It looks like a line would be a reasonable model for this data. In reality, you would have more information about the dataset so you might choose a model based on physics or other reasoning. But here, let's continue this example with just fitting a line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, it is helpful to define a python function that computes model `y` values for given parameter values and input `x` data points. Here's a skeleton for you to write your own function to compute the model. `params` should be an array of model parameters, perhaps `[slope, intercept]`, and `x` should be an array of the actual `x` points where you want to compute the model values. Your function should return the model values in `model_values`.\n",
    "\n",
    "Note: In general, these functions you define should be as efficient as possible. They may be called thousands or millions of times in an MCMC run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your model\n",
    "def compute_model(params,x):\n",
    "    # Compute model_values here\n",
    "    \n",
    "    return model_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test your model\n",
    "Although not necessary, it's always good to make sure you didn't choose something ridiculuous for your model or you didn't make some mistake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the model guess\n",
    "params_guess = [5,45] # slope, intercept guess\n",
    "x_arr = np.linspace(0,10,11) # an array of 11 elements from 0 to 10, for plotting\n",
    "# Compute model values\n",
    "model_arr = compute_model(params_guess,x_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data and model guess\n",
    "plt.figure()\n",
    "plt.errorbar(x,y,yerr=y_err,fmt='bo',label='data'); # plot the data\n",
    "plt.plot(x_arr,model_arr,'r-',label='model guess'); # plot the model guess\n",
    "plt.legend()\n",
    "plt.xlabel('x');\n",
    "plt.ylabel('y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Construct the log likelihood function\n",
    "Next, let's define a python function that will return the value of the likelihood, i.e. the probability of measuring the dataset given the model parameters, `prob(data|model)`. However, during these computations, you may be comparing very small differences in probabilities, resulting in errors due to numerical underflow. So, it is much easier to work with (natural) logarithms of probabilities instead. \n",
    "\n",
    "Below, fill in the cell to complete the function. You will return the natural log of the likelihood probability. For use with `emcee`, the first argument must be the array of parameters. The other arguments you will need to pass will be your data. Here, let's assume the data are measured so that the x points have no errorbars and the y points have Guassian errorbars. Also, let's assume each data point is independently measured.\n",
    "\n",
    "Note: I would recommend using a call to `get_model` that you defined above so that if you do want to change the model, you can just swap out a different function. However, you can of course just compute the model again inside this function. And as before, this function will be called thousands to millions (or more) times, so think about efficiency (i.e. avoid loops! take advantage of vectorized methods)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your likelihood function\n",
    "def get_ln_like(params,x,y,yerr):\n",
    "    # fill in your function here\n",
    "    \n",
    "    return ln_like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Construct the log prior function\n",
    "The next step is to define a python function that returns the value of the prior probability, i.e. the probability of a given model being true, `prob(model)`. Again, it is easier to work with logarithms of probabilities.\n",
    "\n",
    "Let's start with an uninformed (uniform) prior. Fill in the cell below to create a function that returns the value of the prior probability for various models. Note that these probabilities do not depend on the dataset at all, the only input you need is the model parameters, which is why these are \"prior\" probabilities, i.e. before you consider any data.\n",
    "\n",
    "Hint: Although a true uninformed prior would encompass all real values, it is not really practical to compute infinite ranges. So, as discussed in today's session, pick upper/lower limits on the range of the two model parameters.\n",
    "\n",
    "Note: For this exercise, please do ensure your returned prior value is normalized so that the sum over all possible adds up to one. But, note that in practice, this is not really necessary because in today's session, we are only interested in relative posterior probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define uninformed prior\n",
    "def get_ln_prior(params):\n",
    "    # fill in your function here\n",
    "\n",
    "    return ln_prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Construct the log posterior function\n",
    "Finally, we can calculate the posterior probability, which is our actual goal: the probability of the model given the data and priors, i.e. `prob(model|data) ~ prob(data|model) * prob(model)`.\n",
    "\n",
    "Again, we want to work in log space. We will need to compute `ln_like` and the `ln_prior` so we need both the `params` array and all of the dataset. Fill in the cell below to compute the `ln_post` from `ln_like` and `ln_prior`, using your functions defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define posterior\n",
    "def get_ln_post(params,x,y,yerr):\n",
    "    # fill in your function here\n",
    "\n",
    "    return ln_post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test it out!\n",
    "Let's just plug in some numbers to see if we're getting results that make sense. In the cell below, change the initial guess around to have some good guesses and bad guesses and see that when the fit goes bad, the posterior probability decreases. Run the cell after each guess.\n",
    "\n",
    "Be sure to try some guesses outside of whatever range you defined as your prior above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the model guess\n",
    "params_guess = [5,50] # slope, intercept guess: CHANGE THIS\n",
    "x_arr = np.linspace(0,10,11) # an array of 11 elements from 0 to 10, for plotting\n",
    "model_arr = compute_model(params_guess,x_arr)\n",
    "# Plot the data and model guess\n",
    "plt.figure()\n",
    "plt.errorbar(x,y,yerr=y_err,fmt='bo',label='data'); # plot the data\n",
    "plt.plot(x_arr,model_arr,'r-',label='model guess'); # plot the model guess\n",
    "plt.legend()\n",
    "plt.xlabel('x');\n",
    "plt.ylabel('y');\n",
    "# Print the likelihood, prior, and posterior for the guess\n",
    "ln_like  = get_ln_like(params_guess,x,y,y_err)\n",
    "ln_prior = get_ln_prior(params_guess)\n",
    "ln_post  = get_ln_post(params_guess,x,y,y_err)\n",
    "print('ln_like  = {:+.3f}, like prob  = {}'.format(ln_like,np.exp(ln_like)))\n",
    "print('ln_prior = {:+.3f}, prior prob = {}'.format(ln_prior,np.exp(ln_prior)))\n",
    "print('ln_post  = {:+.3f}, post prob  = {}'.format(ln_post,np.exp(ln_post)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Sampling the posterior with MCMC\n",
    "\n",
    "## Brute force won't work!\n",
    "In the preceding steps, we see that we can use Bayes Theorem to calculate the posterior because we know how to calculate the likelihood and we get to choose the prior. In the final step above, we have a little piece of code that can tell us the posterior for any pair of model parameters. So, naively, if we wanted to sample the posterior and map it out, we can do this with brute force! That is, we can set up a fine grid for each of the two parameters and meticulously step through each grid point, calculate the posterior and write it down. If we do this (or get a computer to do this and plot it), we can draw a map of the posterior! However, this is not very efficient at all. It may be doable with just two dimensions and a small range of valid parameter values. But this will eventually require N^M calculations, where N is the number of grid points per parameter and M is the number of parameters. A coarse grid of just 50 points is already over 6 million calcualtions with only 4 parameters!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A smarter way: MCMC with `emcee`\n",
    "As mentioned before, MCMC is a smarter way to compute the posterior. We want to be spending our time computing the posterior (\"sampling\") in areas of high probability, not the entire grid which will mostly be empty. Here, we will be implementing the `emcee` sampler. For this exercise, we won't look into the black box and just focus on getting results. For more information on this package, check out: http://dfm.io/emcee/current/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the package\n",
    "import emcee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the EnsembleSampler object\n",
    "To set up the emcee EnsembleSampler object, we need to tell it how many dimensions we're working in and how many walkers to use to explore the parameter space. The number of dimensions, `ndim` is just the number of parameters. Each walker starts at an initial guess and using the black box algorithm, explores the posterior space and collects samples. This is an \"EnsembleSampler\" because each walker is a member of the \"Ensemble\", so the combination of samples from all the walkers will constitute our final MCMC \"chain\". The number of walkers, `nwalkers` should be even and at least twice the number of dimensions. Typically, a large number such as 100 is a good choice, maybe more for more complicated problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the emcee sampler\n",
    "ndim=2\n",
    "nwalkers=100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the `emcee.EnsembleSampler` object, which takes 3 required arguments plus some optional ones. The three required arguments, in order, are the number of walkers, the number of dimensions, and a function that takes a vector of parameters as input and returns the log of the posterior probability (i.e. what we did above). There are also some optional arguments, we will be using the `args` optional argument in order to pass additional arguments to the posterior function (by default, `emcee` will only pass it the vector of model parameters).\n",
    "\n",
    "For more on what you can do, check out the API: http://dfm.io/emcee/current/api/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the EnsembleSampler object using the posterior\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, get_ln_post, args=[x,y,y_err])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the sampler from an initial guess\n",
    "Now, we just have to start drawing samples! We must first start with an initial point for all 100 walkers. This looks like 100 pairs of `(slope,intercept)` points in this case. In general, it should have shape `(nwalkers,ndim)`. `emcee` provides a utility that generates a ball of points around some initial guess with some standard deviation you provide. The following line creates an initial point for all walkers centered on a slope of 5.0, an intercept of 45 and standard deviations of 2.0 for the slope and 10 for the intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initital position of walkers\n",
    "p0 = emcee.utils.sample_ball((5.0,45),(2.0,10),nwalkers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, run the sampler for 1000 steps (100,000 samples in total), starting from this initial position\n",
    "sampler.run_mcmc(p0,1000);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse results of MCMC run\n",
    "Now that we have our 100,000 samples, let's take a look at time and make some calculations / do some analysis. The `sampler` object contains some useful attributes. First, the `chain` attribute (`sampler.chain`) has shape `(nwalkers,nsteps,ndim)` which allows you to see the value for each dimension/parameter at each step for each walker. For statistics including all the walkers, it is more useful to use `sampler.flatchain` which flattens out the `nwalkers` dimension, so it just has shape `(nsteps,ndim)`. In the way we set up our problem, the first dimension is the slope while the second dimension is the intercept. So, we can plot histograms and calculate some values from the samples of these two parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of how to get the samples for individual parameters\n",
    "slopes = sampler.flatchain[:,0] # all 100,000 samples of slope\n",
    "intercepts = sampler.flatchain[:,1] # all 100,000 samples of intercept\n",
    "# Calculate and print median and standard deviation, representing 50th percentile value and the \"spread\"\n",
    "median_values = np.median(sampler.flatchain,axis=0)\n",
    "slope_median, intercept_median = median_values\n",
    "stddev_values = np.std(sampler.flatchain,axis=0)\n",
    "slope_std, intercept_std = stddev_values\n",
    "print('Median and stddev for slope are: {:.3f} +/- {:.3f}'.format(slope_median,slope_std))\n",
    "print('Median and stddev for intercept are: {:.3f} +/- {:.3f}'.format(intercept_median,intercept_std))\n",
    "# Now plot the histograms\n",
    "plt.figure()\n",
    "plt.subplot(1,2,1)\n",
    "plt.hist(slopes,bins=100);\n",
    "plt.title('slope samples')\n",
    "plt.subplot(1,2,2)\n",
    "plt.hist(intercepts,bins=100);\n",
    "plt.title('intercept samples')\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plots are a demostration of what is in the chain object. The author of `emcee` also wrote a program called `corner` to plot the distributions of the samples as a matrix of pairwise correlations that make it easier to visualize the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import corner\n",
    "import corner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using corner is easy, just supply it with the `flatchain` object for a quick look. Below, there are some optional parameters added for some details. You can see all your options by checking out the corner.py API: https://corner.readthedocs.io/en/latest/api.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corner.corner(sampler.flatchain, labels=['slope','intercept']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better display the samples, you can use the `range` argument to choose the limits on the plots (Note: This doesn't change your MCMC output, just how this set of plots is displayed!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corner.corner(sampler.flatchain, labels=['slope','intercept'], range=((1,7),(35,60)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another useful argument is `truths` which draws crosshairs over whatever values you want (here, we do the medians)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add in some lines for the median values \"truths\"\n",
    "corner.corner(sampler.flatchain, labels=['slope','intercept'], range=((1,7),(35,60)), truths=median_values);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the last corner.py demostration, let's add lines for other quantiles. For instance, we can draw lines on the histograms for the 16th percentile and the 84th percentile. These quantiles encompass 68% of the probability, so they are representative of the 68% confidence interval centered on the median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add more lines for the 16th and 84th percentiles\n",
    "corner.corner(sampler.flatchain, labels=['slope','intercept'], range=((1,7),(35,60)), truths=median_values, quantiles=[0.16,0.84]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, maybe you are interested in the most likely value rather than the median. Often, in Bayesian infernece, the most likely value does not have much meaning as we are more interested in the overall distribution of the posterior. \n",
    "\n",
    "When you have a Gaussian posterior, like we do here, the median is also the most likely value (\"best fit\"). But in general, we can find the most likely value by finding the highest value in the `sampler.flatlnprobability` array and getting the corresponding entry in `sampler.flatchain`. Try to do this below. Hint: Use `np.argmax()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the most likely value from MCMC chain\n",
    "slopes = sampler.flatchain[:,0] # all samples of slope\n",
    "intercepts = sampler.flatchain[:,1] # all samples of intercept\n",
    "step_max = np.argmax(sampler.flatlnprobability) # The step in the chain with the highest post. probabiblity\n",
    "# Print out the answers\n",
    "print('The most likely slope is {:.3f}'.format(slopes[step_max]))\n",
    "print('The most likely intercept is {:.3f}'.format(intercepts[step_max]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final MCMC remarks\n",
    "Now you have the basic framework for implementing `emcee`. Because you have written all the code to be modular, you can swap out each function to meet your own research needs. You'd load up your own data. You can redefine the model function to be whatever model you are trying to use. You can change how the priors are defined. You can modify the `emcee.EnsembleSampler` setup to match your research question!\n",
    "\n",
    "**Optional exercise:** Go back and modify the prior to be something really restrictive and see how the posteriors turn out!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Comparison with other methods\n",
    "With a least squares fit for a line, there is actually an analytical solution. It's not in the scope of this workshop, but you can check out Section X.X in Sivia, or just Google it :) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also a way to numerically solve for the most likely value by using `scipy.optimize` routines on the `lnlike` or `lnpost` functions above, giving you the maximum likelihood estimate and the maximum a posteriori (MAP) estimate, respectively. The difference between the two is whether or not to include the prior. These methods are estimates for the mode of the likelihood and posterior pdfs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import scipy's optimization module\n",
    "import scipy.optimize as op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of these numerical methods actually find minimums, not maximums. So, using the `lambda` notation, we can quickly create anonymous functions that returns the negative log likelihood and negative log posterior for this part. Then, we use `scipy.opitmize.minimize` with an initial guess for parameters (5,45) and pass it the data. The results are stored in the `results` object and the attribute corresponding to the most likely parameters is `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neglnlike = lambda *args: -get_ln_like(*args)\n",
    "result = op.minimize(neglnlike, [5,45], args=(x, y, y_err))\n",
    "slope_maxlike, intercept_maxlike = result[\"x\"]\n",
    "print('Max likelihood estimates for slope     = {:.3f}'.format(slope_maxlike))\n",
    "print('Max likelihood estimates for intercept = {:.3f}'.format(intercept_maxlike))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another useful attribute from `result` is `result['hess_inv']`. The inverse Hessian is basically a covariance matrix. If you are not familiar with these, one useful piece of info is that if you read off the diagonals, you are getting the variances (square of standard deviation) for the model parameters, so you can estimate the spread of your posterior on each parameter by taking the square root of the diagonals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result['hess_inv'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the following to the MCMC results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_like_stddevs = np.sqrt(np.diag(result['hess_inv']))\n",
    "slope_stddev_maxlike, intercept_stddev_maxlike = max_like_stddevs\n",
    "print('Max likelihood estimate results:')\n",
    "print('Slope:     {:.3f} +/- {:.3f}'.format(slope_maxlike,slope_stddev_maxlike))\n",
    "print('Intercept: {:.3f} +/- {:.3f}'.format(intercept_maxlike,intercept_stddev_maxlike))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum a posteriori (MAP) estimates\n",
    "Since our priors are uniform, they don't really influence the posterior, so we expect the answer to be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neglnpost = lambda *args: -get_ln_post(*args)\n",
    "result_MAP = op.minimize(neglnpost, [5,45], args=(x, y, y_err))\n",
    "slope_MAP, intercept_MAP = result_MAP[\"x\"]\n",
    "MAP_stddevs = np.sqrt(np.diag(result_MAP['hess_inv']))\n",
    "slope_stddev_MAP, intercept_stddev_MAP = MAP_stddevs\n",
    "print('Maximum a posteriori results:')\n",
    "print('Slope:     {:.3f} +/- {:.3f}'.format(slope_MAP,slope_stddev_MAP))\n",
    "print('Intercept: {:.3f} +/- {:.3f}'.format(intercept_MAP,intercept_stddev_MAP))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpful hint: Using maximum likelihood or maximum a posteriori estimates with MCMC\n",
    "For complicated problems where you cannot just \"eyeball\" an initial guess, you can start your MCMC from your maximum likelihood/posterior value and use the covariance matrix to compute a good standard deviation to intialize your ball of walker starting points."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
